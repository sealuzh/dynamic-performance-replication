/*
 * This file was automatically generated by EvoSuite
 * Fri Jul 06 17:35:16 GMT 2018
 */

package weka.classifiers;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.util.Arrays;
import java.util.Enumeration;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.classifiers.CheckClassifier;
import weka.classifiers.functions.SGDText;
import weka.classifiers.lazy.LWL;
import weka.core.Option;
import weka.core.tokenizers.WordTokenizer;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class CheckClassifier_ESTest extends CheckClassifier_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      String[] stringArray0 = checkClassifier0.getOptions();
      LWL lWL0 = new LWL();
      CheckClassifier.main(stringArray0);
  }

  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      // Undeclared exception!
      try { 
        checkClassifier0.updatingEquality(true, false, false, true, false, true, (-1));
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '-1' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      checkClassifier0.doesntUseTestClassVal(false, false, false, false, false, false, 0);
      CheckClassifier checkClassifier1 = new CheckClassifier();
      boolean[] booleanArray0 = checkClassifier1.canTakeOptions();
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
  }

  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      String[] stringArray0 = new String[2];
      stringArray0[0] = "22Ol+3]m|q";
      stringArray0[1] = "";
      CheckClassifier.main(stringArray0);
  }

  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      checkClassifier0.multiInstanceHandler();
      CheckClassifier checkClassifier1 = new CheckClassifier();
      Enumeration enumeration0 = checkClassifier1.listOptions();
      assertNotNull(enumeration0);
  }

  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      // Undeclared exception!
      try { 
        checkClassifier0.instanceWeights(false, true, false, false, false, false, (-288));
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '-288' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      checkClassifier0.canTakeOptions();
      // Undeclared exception!
      try { 
        checkClassifier0.correctBuildInitialisation(false, false, false, false, false, true, 2749);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '2749' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      // Undeclared exception!
      try { 
        checkClassifier0.correctBuildInitialisation(true, true, false, true, true, false, 93);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '93' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      checkClassifier0.declaresSerialVersionUID();
      // Undeclared exception!
      try { 
        checkClassifier0.datasetIntegrity(false, true, true, true, false, false, 998, false, true);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '998' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      boolean[] booleanArray0 = checkClassifier0.testToString();
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      
      CheckClassifier checkClassifier1 = new CheckClassifier();
      boolean[] booleanArray1 = checkClassifier1.datasetIntegrity(true, false, false, false, true, false, 4, true, true);
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray1));
  }

  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertNotNull(checkClassifier0);
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertNotNull(stringArray0);
      
      // Undeclared exception!
      try { 
        checkClassifier0.instanceWeights(false, false, false, true, false, true, 32);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '32' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertNotNull(checkClassifier0);
      
      checkClassifier0.setDebug(false);
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      
      boolean[] booleanArray0 = checkClassifier0.instanceWeights(false, false, false, false, false, false, 0);
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      String string0 = checkClassifier0.getWords();
      assertEquals("The,quick,brown,fox,jumps,over,the,lazy,dog", string0);
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertNotNull(string0);
      
      boolean[] booleanArray1 = checkClassifier0.updateableClassifier();
      assertFalse(booleanArray1.equals((Object)booleanArray0));
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray1));
      assertNotNull(booleanArray1);
      assertNotSame(booleanArray1, booleanArray0);
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertNotNull(stringArray0);
      
      CheckClassifier checkClassifier1 = new CheckClassifier();
      assertFalse(checkClassifier1.equals((Object)checkClassifier0));
      assertEquals(2, checkClassifier1.getNumNominal());
      assertFalse(checkClassifier1.hasClasspathProblems());
      assertEquals(1, checkClassifier1.getNumString());
      assertFalse(checkClassifier1.getDebug());
      assertEquals(1, checkClassifier1.getNumDate());
      assertEquals(1, checkClassifier1.getNumNumeric());
      assertEquals(10, checkClassifier1.getNumInstancesRelational());
      assertFalse(checkClassifier1.getSilent());
      assertEquals(1, checkClassifier1.getNumRelational());
      assertEquals(20, checkClassifier1.getNumInstances());
      assertEquals(" ", checkClassifier1.getWordSeparators());
      assertNotNull(checkClassifier1);
      
      boolean[] booleanArray2 = checkClassifier0.canTakeOptions();
      assertFalse(checkClassifier0.equals((Object)checkClassifier1));
      assertFalse(booleanArray2.equals((Object)booleanArray0));
      assertFalse(booleanArray2.equals((Object)booleanArray1));
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray2));
      assertNotNull(booleanArray2);
      assertNotSame(checkClassifier0, checkClassifier1);
      assertNotSame(booleanArray2, booleanArray0);
      assertNotSame(booleanArray2, booleanArray1);
  }

  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getSilent());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.multiInstanceHandler();
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getSilent());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      CheckClassifier checkClassifier1 = new CheckClassifier();
      assertFalse(checkClassifier1.equals((Object)checkClassifier0));
      assertEquals(10, checkClassifier1.getNumInstancesRelational());
      assertFalse(checkClassifier1.getSilent());
      assertEquals(1, checkClassifier1.getNumString());
      assertEquals(1, checkClassifier1.getNumRelational());
      assertFalse(checkClassifier1.hasClasspathProblems());
      assertEquals(" ", checkClassifier1.getWordSeparators());
      assertEquals(2, checkClassifier1.getNumNominal());
      assertFalse(checkClassifier1.getDebug());
      assertEquals(1, checkClassifier1.getNumDate());
      assertEquals(1, checkClassifier1.getNumNumeric());
      assertEquals(20, checkClassifier1.getNumInstances());
      assertNotNull(checkClassifier1);
      
      String string0 = checkClassifier1.getRevision();
      assertEquals("8034", string0);
      assertFalse(checkClassifier1.equals((Object)checkClassifier0));
      assertEquals(10, checkClassifier1.getNumInstancesRelational());
      assertFalse(checkClassifier1.getSilent());
      assertEquals(1, checkClassifier1.getNumString());
      assertEquals(1, checkClassifier1.getNumRelational());
      assertFalse(checkClassifier1.hasClasspathProblems());
      assertEquals(" ", checkClassifier1.getWordSeparators());
      assertEquals(2, checkClassifier1.getNumNominal());
      assertFalse(checkClassifier1.getDebug());
      assertEquals(1, checkClassifier1.getNumDate());
      assertEquals(1, checkClassifier1.getNumNumeric());
      assertEquals(20, checkClassifier1.getNumInstances());
      assertNotNull(string0);
      assertNotSame(checkClassifier1, checkClassifier0);
  }

  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertNotNull(checkClassifier0);
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertNotNull(stringArray0);
      
      LWL lWL0 = new LWL();
      assertEquals(0, LWL.LINEAR);
      assertEquals(2, LWL.TRICUBE);
      assertEquals(3, LWL.INVERSE);
      assertEquals(5, LWL.CONSTANT);
      assertEquals(4, LWL.GAUSS);
      assertEquals(1, LWL.EPANECHNIKOV);
      assertEquals("The base classifier to be used.", lWL0.classifierTipText());
      assertEquals("How many neighbours are used to determine the width of the weighting function (<= 0 means all neighbours).", lWL0.KNNTipText());
      assertEquals(0, lWL0.getWeightingKernel());
      assertEquals("Determines weighting function. [0 = Linear, 1 = Epnechnikov,2 = Tricube, 3 = Inverse, 4 = Gaussian and 5 = Constant. (default 0 = Linear)].", lWL0.weightingKernelTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: LinearNN).", lWL0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(lWL0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", lWL0.debugTipText());
      assertEquals((-1), lWL0.getKNN());
      assertNotNull(lWL0);
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/mnt/gaiagpfs/users/homedirs/apanichella/Evosuite_performance/Dataset/gordon_script_mosa/projects/9_weka/Capabilities.props");
      boolean boolean1 = FileSystemHandling.createFolder(evoSuiteFile0);
      assertTrue(boolean1);
      assertTrue(boolean1 == boolean0);
      
      checkClassifier0.testsPerClassType(2, false, false, false);
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      
      // Undeclared exception!
      try { 
        checkClassifier0.correctBuildInitialisation(false, false, true, false, false, true, 2);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Loop has been executed more times than the allowed 10000
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      String[] stringArray0 = new String[7];
      stringArray0[0] = "Buffer is null!";
      stringArray0[1] = "RMQ-llp";
      stringArray0[2] = "JW}!@<";
      stringArray0[3] = "rw'>&|@aa";
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.correctBuildInitialisation(false, false, false, false, false, true, 0);
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      try { 
        checkClassifier0.makeTestDataset(24, 0, 0, 24, 24, 566, (-328), 1410, 0, (-439), false);
        fail("Expecting exception: NegativeArraySizeException");
      
      } catch(NegativeArraySizeException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.TestInstances", e);
      }
  }

  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(checkClassifier0);
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(stringArray0);
      
      LWL lWL0 = new LWL();
      assertEquals(3, LWL.INVERSE);
      assertEquals(2, LWL.TRICUBE);
      assertEquals(4, LWL.GAUSS);
      assertEquals(0, LWL.LINEAR);
      assertEquals(5, LWL.CONSTANT);
      assertEquals(1, LWL.EPANECHNIKOV);
      assertEquals("If set to true, classifier may output additional info to the console.", lWL0.debugTipText());
      assertEquals("The base classifier to be used.", lWL0.classifierTipText());
      assertEquals(0, lWL0.getWeightingKernel());
      assertEquals("Determines weighting function. [0 = Linear, 1 = Epnechnikov,2 = Tricube, 3 = Inverse, 4 = Gaussian and 5 = Constant. (default 0 = Linear)].", lWL0.weightingKernelTipText());
      assertEquals("How many neighbours are used to determine the width of the weighting function (<= 0 means all neighbours).", lWL0.KNNTipText());
      assertFalse(lWL0.getDebug());
      assertEquals("The nearest neighbour search algorithm to use (Default: LinearNN).", lWL0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals((-1), lWL0.getKNN());
      assertNotNull(lWL0);
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/mnt/gaiagpfs/users/homedirs/apanichella/Evosuite_performance/Dataset/gordon_script_mosa/projects/9_weka/Capabilities.props");
      boolean boolean0 = FileSystemHandling.createFolder(evoSuiteFile0);
      assertTrue(boolean0);
      
      checkClassifier0.testsPerClassType(2, false, false, false);
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      
      boolean[] booleanArray0 = checkClassifier0.correctBuildInitialisation(false, true, true, false, false, false, 2);
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray0));
      assertNotNull(booleanArray0);
  }

  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(checkClassifier0);
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(stringArray0);
      
      LWL lWL0 = new LWL();
      assertEquals(4, LWL.GAUSS);
      assertEquals(0, LWL.LINEAR);
      assertEquals(3, LWL.INVERSE);
      assertEquals(2, LWL.TRICUBE);
      assertEquals(1, LWL.EPANECHNIKOV);
      assertEquals(5, LWL.CONSTANT);
      assertEquals("The base classifier to be used.", lWL0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lWL0.debugTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: LinearNN).", lWL0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals((-1), lWL0.getKNN());
      assertEquals("How many neighbours are used to determine the width of the weighting function (<= 0 means all neighbours).", lWL0.KNNTipText());
      assertEquals(0, lWL0.getWeightingKernel());
      assertFalse(lWL0.getDebug());
      assertEquals("Determines weighting function. [0 = Linear, 1 = Epnechnikov,2 = Tricube, 3 = Inverse, 4 = Gaussian and 5 = Constant. (default 0 = Linear)].", lWL0.weightingKernelTipText());
      assertNotNull(lWL0);
      
      checkClassifier0.testsPerClassType(2, false, false, false);
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      
      CheckClassifier checkClassifier1 = new CheckClassifier();
      assertFalse(checkClassifier1.equals((Object)checkClassifier0));
      assertEquals(1, checkClassifier1.getNumRelational());
      assertEquals(20, checkClassifier1.getNumInstances());
      assertEquals(" ", checkClassifier1.getWordSeparators());
      assertEquals(1, checkClassifier1.getNumDate());
      assertEquals(1, checkClassifier1.getNumNumeric());
      assertFalse(checkClassifier1.getSilent());
      assertEquals(10, checkClassifier1.getNumInstancesRelational());
      assertFalse(checkClassifier1.hasClasspathProblems());
      assertFalse(checkClassifier1.getDebug());
      assertEquals(2, checkClassifier1.getNumNominal());
      assertEquals(1, checkClassifier1.getNumString());
      assertNotNull(checkClassifier1);
      
      boolean[] booleanArray0 = checkClassifier0.canTakeOptions();
      assertFalse(checkClassifier0.equals((Object)checkClassifier1));
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      assertNotSame(checkClassifier0, checkClassifier1);
  }

  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.doesntUseTestClassVal(false, false, false, false, false, false, 0);
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      boolean[] booleanArray1 = checkClassifier0.datasetIntegrity(false, false, false, false, true, false, 0, true, false);
      assertFalse(booleanArray1.equals((Object)booleanArray0));
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray1));
      assertNotNull(booleanArray1);
      assertNotSame(booleanArray1, booleanArray0);
  }

  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertNotNull(checkClassifier0);
      
      checkClassifier0.setWordSeparators("(}V");
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals("(}V", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      
      boolean[] booleanArray0 = checkClassifier0.datasetIntegrity(false, false, true, false, false, false, 0, false, false);
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals("(}V", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      boolean[] booleanArray1 = checkClassifier0.canHandleOnlyClass(true, true, false, true, true, 0);
      assertFalse(booleanArray1.equals((Object)booleanArray0));
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals("(}V", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray1));
      assertNotNull(booleanArray1);
      assertNotSame(booleanArray1, booleanArray0);
      
      // Undeclared exception!
      try { 
        checkClassifier0.datasetIntegrity(false, false, false, false, true, true, 0, true, true);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Loop has been executed more times than the allowed 10000
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.weightedInstancesHandler();
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      checkClassifier0.setNumDate(229);
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumNumeric());
      
      checkClassifier0.setDebug(true);
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumNumeric());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertNotNull(sGDText0);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)sGDText0.getTokenizer();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertNotNull(wordTokenizer0);
      
      Enumeration<Option> enumeration0 = sGDText0.listOptions();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertNotNull(enumeration0);
      
      sGDText0.setLearningRate(229);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      
      sGDText0.setTokenizer(wordTokenizer0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      
      sGDText0.reset();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      
      checkClassifier0.setClassifier(sGDText0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(stringArray0);
      
      boolean[] booleanArray1 = checkClassifier0.declaresSerialVersionUID();
      assertFalse(booleanArray1.equals((Object)booleanArray0));
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray1));
      assertNotNull(booleanArray1);
      assertNotSame(booleanArray1, booleanArray0);
      
      String string0 = checkClassifier0.getRevision();
      assertEquals("8034", string0);
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(string0);
      
      boolean[] booleanArray2 = checkClassifier0.canHandleNClasses(false, false, false, false, false, false, 2781);
      assertFalse(booleanArray2.equals((Object)booleanArray0));
      assertFalse(booleanArray2.equals((Object)booleanArray1));
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {false, true}, booleanArray2));
      assertNotNull(booleanArray2);
      assertNotSame(booleanArray2, booleanArray0);
      assertNotSame(booleanArray2, booleanArray1);
      
      boolean[] booleanArray3 = checkClassifier0.correctBuildInitialisation(false, false, false, false, false, true, 0);
      assertFalse(booleanArray3.equals((Object)booleanArray2));
      assertFalse(booleanArray3.equals((Object)booleanArray1));
      assertFalse(booleanArray3.equals((Object)booleanArray0));
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray3));
      assertNotNull(booleanArray3);
      assertNotSame(booleanArray3, booleanArray2);
      assertNotSame(booleanArray3, booleanArray1);
      assertNotSame(booleanArray3, booleanArray0);
      
      // Undeclared exception!
      try { 
        checkClassifier0.canHandleOnlyClass(false, false, true, true, true, 2781);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '2781' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.weightedInstancesHandler();
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      checkClassifier0.setNumDate(229);
      assertEquals(2, checkClassifier0.getNumNominal());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(229, checkClassifier0.getNumDate());
      
      checkClassifier0.setDebug(true);
      assertEquals(2, checkClassifier0.getNumNominal());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(229, checkClassifier0.getNumDate());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertNotNull(sGDText0);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)sGDText0.getTokenizer();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertNotNull(wordTokenizer0);
      
      sGDText0.setLearningRate(229);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      
      sGDText0.setTokenizer(wordTokenizer0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      sGDText0.reset();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      
      checkClassifier0.setClassifier(sGDText0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(2, checkClassifier0.getNumNominal());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(2, checkClassifier0.getNumNominal());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(229, checkClassifier0.getNumDate());
      assertNotNull(stringArray0);
      
      boolean[] booleanArray1 = checkClassifier0.declaresSerialVersionUID();
      assertFalse(booleanArray1.equals((Object)booleanArray0));
      assertEquals(2, checkClassifier0.getNumNominal());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(229, checkClassifier0.getNumDate());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray1));
      assertNotNull(booleanArray1);
      assertNotSame(booleanArray1, booleanArray0);
      
      boolean[] booleanArray2 = checkClassifier0.canHandleOnlyClass(false, false, false, false, true, 1);
      assertFalse(booleanArray2.equals((Object)booleanArray1));
      assertFalse(booleanArray2.equals((Object)booleanArray0));
      assertEquals(2, checkClassifier0.getNumNominal());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(229, checkClassifier0.getNumDate());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray2));
      assertNotNull(booleanArray2);
      assertNotSame(booleanArray2, booleanArray1);
      assertNotSame(booleanArray2, booleanArray0);
      
      boolean[] booleanArray3 = checkClassifier0.canTakeOptions();
      assertFalse(booleanArray3.equals((Object)booleanArray2));
      assertFalse(booleanArray3.equals((Object)booleanArray0));
      assertFalse(booleanArray3.equals((Object)booleanArray1));
      assertEquals(2, checkClassifier0.getNumNominal());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(229, checkClassifier0.getNumDate());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray3));
      assertNotNull(booleanArray3);
      assertNotSame(booleanArray3, booleanArray2);
      assertNotSame(booleanArray3, booleanArray0);
      assertNotSame(booleanArray3, booleanArray1);
  }

  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.weightedInstancesHandler();
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      checkClassifier0.setNumDate(229);
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      
      checkClassifier0.setDebug(true);
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertNotNull(sGDText0);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)sGDText0.getTokenizer();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertNotNull(wordTokenizer0);
      
      sGDText0.setLearningRate(229);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      
      sGDText0.setTokenizer(wordTokenizer0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      sGDText0.reset();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      
      checkClassifier0.setClassifier(sGDText0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertNotNull(stringArray0);
      
      boolean[] booleanArray1 = checkClassifier0.declaresSerialVersionUID();
      assertFalse(booleanArray1.equals((Object)booleanArray0));
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray1));
      assertNotNull(booleanArray1);
      assertNotSame(booleanArray1, booleanArray0);
      
      boolean[] booleanArray2 = checkClassifier0.canHandleOnlyClass(false, false, false, false, true, 1);
      assertFalse(booleanArray2.equals((Object)booleanArray0));
      assertFalse(booleanArray2.equals((Object)booleanArray1));
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray2));
      assertNotNull(booleanArray2);
      assertNotSame(booleanArray2, booleanArray0);
      assertNotSame(booleanArray2, booleanArray1);
      
      Enumeration enumeration0 = checkClassifier0.listOptions();
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertNotNull(enumeration0);
      
      boolean[] booleanArray3 = checkClassifier0.canHandleNClasses(false, false, false, false, false, true, 25);
      assertFalse(booleanArray3.equals((Object)booleanArray0));
      assertFalse(booleanArray3.equals((Object)booleanArray2));
      assertFalse(booleanArray3.equals((Object)booleanArray1));
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(checkClassifier0.getDebug());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertTrue(Arrays.equals(new boolean[] {false, true}, booleanArray3));
      assertNotNull(booleanArray3);
      assertNotSame(booleanArray3, booleanArray0);
      assertNotSame(booleanArray3, booleanArray2);
      assertNotSame(booleanArray3, booleanArray1);
  }

  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertNotNull(checkClassifier0);
      
      checkClassifier0.setNumInstancesRelational(2600);
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2600, checkClassifier0.getNumInstancesRelational());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getSilent());
      
      String string0 = checkClassifier0.getWords();
      assertEquals("The,quick,brown,fox,jumps,over,the,lazy,dog", string0);
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2600, checkClassifier0.getNumInstancesRelational());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getSilent());
      assertNotNull(string0);
      
      boolean[] booleanArray0 = checkClassifier0.updateableClassifier();
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2600, checkClassifier0.getNumInstancesRelational());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getSilent());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2600, checkClassifier0.getNumInstancesRelational());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getSilent());
      assertNotNull(stringArray0);
      
      // Undeclared exception!
      try { 
        checkClassifier0.canHandleMissing(false, true, true, false, true, false, (-2062), true, true, (-1122));
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '-2062' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(1, checkClassifier0.getNumString());
      assertNotNull(checkClassifier0);
      
      checkClassifier0.setNumRelational((-130));
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals((-130), checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumString());
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals((-130), checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumString());
      assertNotNull(stringArray0);
      
      LWL lWL0 = new LWL();
      assertEquals(2, LWL.TRICUBE);
      assertEquals(3, LWL.INVERSE);
      assertEquals(1, LWL.EPANECHNIKOV);
      assertEquals(4, LWL.GAUSS);
      assertEquals(0, LWL.LINEAR);
      assertEquals(5, LWL.CONSTANT);
      assertEquals("The base classifier to be used.", lWL0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", lWL0.debugTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: LinearNN).", lWL0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals((-1), lWL0.getKNN());
      assertEquals("Determines weighting function. [0 = Linear, 1 = Epnechnikov,2 = Tricube, 3 = Inverse, 4 = Gaussian and 5 = Constant. (default 0 = Linear)].", lWL0.weightingKernelTipText());
      assertEquals("How many neighbours are used to determine the width of the weighting function (<= 0 means all neighbours).", lWL0.KNNTipText());
      assertFalse(lWL0.getDebug());
      assertEquals(0, lWL0.getWeightingKernel());
      assertNotNull(lWL0);
      
      // Undeclared exception!
      try { 
        checkClassifier0.updatingEquality(false, false, false, true, true, true, 3);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: null
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.updatingEquality(true, false, false, false, false, false, 0);
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      CheckClassifier checkClassifier1 = new CheckClassifier();
      assertFalse(checkClassifier1.equals((Object)checkClassifier0));
      assertEquals(1, checkClassifier1.getNumString());
      assertFalse(checkClassifier1.getDebug());
      assertEquals(10, checkClassifier1.getNumInstancesRelational());
      assertFalse(checkClassifier1.getSilent());
      assertEquals(20, checkClassifier1.getNumInstances());
      assertEquals(" ", checkClassifier1.getWordSeparators());
      assertEquals(1, checkClassifier1.getNumDate());
      assertEquals(1, checkClassifier1.getNumNumeric());
      assertEquals(2, checkClassifier1.getNumNominal());
      assertEquals(1, checkClassifier1.getNumRelational());
      assertFalse(checkClassifier1.hasClasspathProblems());
      assertNotNull(checkClassifier1);
      
      String string0 = checkClassifier0.getRevision();
      assertEquals("8034", string0);
      assertFalse(checkClassifier0.equals((Object)checkClassifier1));
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertNotNull(string0);
      assertNotSame(checkClassifier0, checkClassifier1);
  }

  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumDate());
      assertNotNull(checkClassifier0);
      
      // Undeclared exception!
      try { 
        checkClassifier0.updatingEquality(true, true, true, true, true, false, (-1));
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '-1' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.testToString();
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      checkClassifier0.printAttributeSummary(true, false, false, false, true, false, 21);
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(20, checkClassifier0.getNumInstances());
      
      // Undeclared exception!
      try { 
        checkClassifier0.canHandleClassAsNthAttribute(true, true, true, true, true, false, 21, (-1));
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '21' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(checkClassifier0);
      
      int int0 = 0;
      boolean[] booleanArray0 = checkClassifier0.updatingEquality(true, false, false, true, true, false, 0);
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      int int1 = 2378;
      // Undeclared exception!
      try { 
        checkClassifier0.correctBuildInitialisation(true, true, true, false, false, false, 2378);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '2378' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      String[] stringArray0 = new String[7];
      stringArray0[0] = "Buffer is null!";
      stringArray0[1] = "RMQ-llp";
      int int0 = 0;
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(checkClassifier0);
      
      boolean boolean0 = false;
      boolean[] booleanArray0 = checkClassifier0.updatingEquality(false, false, false, false, false, false, 0);
      assertFalse(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumDate());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      int int1 = 2378;
      boolean boolean1 = false;
      boolean boolean2 = false;
      boolean boolean3 = true;
      // Undeclared exception!
      try { 
        checkClassifier0.correctBuildInitialisation(false, false, false, false, true, false, 2378);
        fail("Expecting exception: Error");
      
      } catch(Error e) {
         //
         // Error setting up for tests: Attribute type '2378' unknown!
         //
         verifyException("weka.classifiers.CheckClassifier", e);
      }
  }

  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      CheckClassifier checkClassifier0 = new CheckClassifier();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(checkClassifier0);
      
      boolean[] booleanArray0 = checkClassifier0.weightedInstancesHandler();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertEquals(1, checkClassifier0.getNumDate());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray0));
      assertNotNull(booleanArray0);
      
      checkClassifier0.setNumDate(229);
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertFalse(checkClassifier0.getDebug());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      
      checkClassifier0.setDebug(true);
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertNotNull(sGDText0);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)sGDText0.getTokenizer();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertNotNull(wordTokenizer0);
      
      sGDText0.setLearningRate(229);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      
      sGDText0.setTokenizer(wordTokenizer0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      sGDText0.reset();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      
      checkClassifier0.setClassifier(sGDText0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(229.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      
      String[] stringArray0 = checkClassifier0.getOptions();
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertNotNull(stringArray0);
      
      boolean[] booleanArray1 = checkClassifier0.declaresSerialVersionUID();
      assertFalse(booleanArray1.equals((Object)booleanArray0));
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {true, false}, booleanArray1));
      assertNotNull(booleanArray1);
      assertNotSame(booleanArray1, booleanArray0);
      
      boolean[] booleanArray2 = checkClassifier0.datasetIntegrity(false, false, false, false, false, false, 1, true, true);
      assertFalse(booleanArray2.equals((Object)booleanArray1));
      assertFalse(booleanArray2.equals((Object)booleanArray0));
      assertEquals(" ", checkClassifier0.getWordSeparators());
      assertEquals(1, checkClassifier0.getNumRelational());
      assertEquals(20, checkClassifier0.getNumInstances());
      assertTrue(checkClassifier0.getDebug());
      assertFalse(checkClassifier0.hasClasspathProblems());
      assertEquals(10, checkClassifier0.getNumInstancesRelational());
      assertFalse(checkClassifier0.getSilent());
      assertEquals(1, checkClassifier0.getNumString());
      assertEquals(2, checkClassifier0.getNumNominal());
      assertEquals(229, checkClassifier0.getNumDate());
      assertEquals(1, checkClassifier0.getNumNumeric());
      assertTrue(Arrays.equals(new boolean[] {false, false}, booleanArray2));
      assertNotNull(booleanArray2);
      assertNotSame(booleanArray2, booleanArray1);
      assertNotSame(booleanArray2, booleanArray0);
  }
}
