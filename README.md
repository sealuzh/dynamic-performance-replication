# Replication Package

This is the online appendix/replication package for the paper *"Testing with Fewer Resources: An Adaptive Approach to Performance-Aware Test-Case Generation"*.

In this online repository we provide:

* a **runnable version** of the implemented approaches: the *evosuite.jar* file;
* all the [tables][tables] with the full tables reporting the results described in the paper;
* the script used to generate the tests;
* a zipped folders with all the projects used in the empirical study.

### EvoSuite jar
We built a custom EvoSuite version implementing the pDynaMOSA starting from [EvoSuite 1.0.6.][evosuite].
To run the experiments reported in the paper, we relied on the bash scripts that you can find in the _scripts_ directory for every approach tested.
The full source code can be found [here](https://github.com/giograno/evosuite)

### Full Results
#### RQ1/RQ2
[Here][tables] we report the full results for our RQ1/2, i.e., the comparison of pDynaMOSA respectively version the random approach (first baseline) and DynaMOSA (second baseline) across the 7 different criteria as well as for the strong mutation coverage.

#### Running Time
[This table](https://github.com/sealuzh/dynamic-performance-replication/blob/master/tables/running-time.csv) reports the running time comparison (in milliseconds) between MOSA and pMOSA. The reported statistics are the same reported for Table 1.
Moreover, we report also the delta of the coverage for the measured suite for every subject. A positive difference means that the measured test suite generated by pMOSA achieves an higher coverage. A negative difference means that the measured test suite generated by MOSA achieves an higher coverage.
The table reports only the statistically significantly values (p-value < 0.05).

#### Running Time
[This table](https://github.com/sealuzh/dynamic-performance-replication/blob/master/tables/heap-memory.csv) reports the heap memory (in MB) comparison between MOSA and pMOSA. The reported statistics and the delta of the coverage have the same interpretation than Table 2.

[Here](https://github.com/sealuzh/dynamic-performance-replication/blob/master/tables/cov-second-obj.csv) we report the experimental data about branch coverage we achieved with the usage of performance indicators as secondary objective. As explained in the paper, we did not used it since such approach is detrimental for the final coverage.

[evosuite]: https://github.com/EvoSuite/evosuite
[tables]: https://github.com/sealuzh/dynamic-performance-replication/blob/master/tables

### Generated Tests & Raw Data
We include three folders, one for: 

* [MOSA][mosa];
* The approach using the preference criterion as a second objective [here][pmosa];
* [pMOSA][amosa];
 
All the generated tests are in stored in the correspondent _tests_ directories.
The _raw-stats.csv_ files instead, reports the measured statistics regarding the generated tests (like coverage, mutation score, result size, number of generations and so on). It is the csv file that EvoSuite regurarly produce as output.

[mosa]: https://github.com/sealuzh/dynamic-performance-replication/tree/master/generated-tests/mosa
[pmosa]: https://github.com/sealuzh//tree/master/generated-tests/second-obj
[amosa]: https://github.com/sealuzh/dynamic-performance-replication/tree/master/generated-tests/pmosa

### Performance Measurement
The data about the performance measurements are in [this folder][measurements].
We also include two csv files indicating the exact test suites, amongst the generated ones, that has been measured.
Moreover, we include the source code used for the measurement as well the script used to process the raw data.

[measurements]: https://github.com/sealuzh/dynamic-performance-replication/tree/master/performance-measurement/