/*
 * This file was automatically generated by EvoSuite
 * Fri Jul 06 20:21:18 GMT 2018
 */

package weka.classifiers.functions.supportVector;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.util.Enumeration;
import java.util.Locale;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.System;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.evosuite.runtime.util.SystemInUtil;
import org.junit.runner.RunWith;
import weka.attributeSelection.BestFirst;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.BayesNet;
import weka.classifiers.functions.GaussianProcesses;
import weka.classifiers.functions.SGDText;
import weka.classifiers.functions.SMOreg;
import weka.classifiers.functions.supportVector.Kernel;
import weka.classifiers.functions.supportVector.Puk;
import weka.classifiers.functions.supportVector.RegSMO;
import weka.classifiers.functions.supportVector.SMOset;
import weka.classifiers.functions.supportVector.StringKernel;
import weka.classifiers.lazy.LWL;
import weka.classifiers.meta.RegressionByDiscretization;
import weka.core.AbstractInstance;
import weka.core.Capabilities;
import weka.core.Instances;
import weka.core.Option;
import weka.core.SelectedTag;
import weka.core.SparseInstance;
import weka.core.TechnicalInformation;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class RegSMO_ESTest extends RegSMO_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      regSMO0.globalInfo();
      // Undeclared exception!
      try { 
        regSMO0.secondChoiceHeuristic((-2890));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(17, 17, 17, 17, 17, 17, 2647.9, 1909.3, 1.2217239998778275E-7, 2647.9, (-2343.10445718));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      regSMO0.m_nSeed = 104;
      regSMO0.m_alpha1Star = 1.0;
      regSMO0.setEpsilon(1.0);
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(30000, 220.9620049817564, 2.0, 2.0, 30000, 104, 104, 30000, 138.224524347134, 104, 104);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      SystemInUtil.addInputLine("");
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(2666, 1.0, (-0.4375), 1459.63555708125, 50, 50, 0.2, 1.7976931346825464E308, 679.26, 679.26, 1.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      String[] stringArray0 = new String[4];
      RegSMO regSMO0 = new RegSMO();
      System.setCurrentTimeMillis(469L);
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine((-2875), 303.69618, (-575.77242255), 0.0, (-2875), 0.0, 0.0, (-575.77242255), 1651.398263566939, 1651.398263566939, 567.23924998092);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      double[] doubleArray0 = new double[3];
      doubleArray0[1] = 4837.284738348798;
      int int0 = 100;
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(100, 2430.7407713295793, 0.0, 2430.7407713295793, 100, (-0.002), 0.0, 100, (-0.002), 100, 1.0E-12);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      regSMO0.listOptions();
      regSMO0.getOptions();
      BestFirst bestFirst0 = new BestFirst();
      GaussianProcesses gaussianProcesses0 = new GaussianProcesses();
      gaussianProcesses0.getFilterType();
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(2, (-160), 2, 1.0E-10, 1, 2.0, (-160), 288.9, 0.75, 1.0E-10, 1.0E-10);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      regSMO0.m_nCacheHits = (-2614);
      regSMO0.m_nSeed = (-2614);
      regSMO0.getTechnicalInformation();
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(92, 92, 92, 1625.8869639918387, (-2614), 1625.8869639918387, (-2614), 3118.56, 1547.1888, 92, (-2614));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, true, true);
      Puk puk0 = new Puk();
      regSMO0.m_kernel = (Kernel) puk0;
      regSMO0.getTechnicalInformation();
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine((-2873), (-2873), 5639.8031, 5639.8031, (-2873), (-2873), (-0.5), (-2873), 300.0, 5251.290362, 300.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      int int0 = regSMO0.takeStep((-2360), (-2360), (-2360), (-2360), (-2360));
      assertEquals(0, int0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
  }

  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      
      regSMO0.setOptions((String[]) null);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertFalse(regSMO0.modelBuilt());
  }

  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      boolean boolean0 = regSMO0.findOptimalPointOnLine(2098, (-1222.0), (-1222.0), 2098, 2098, 2098, (-1240.258539627861), (-2252.6887925639558), (-3644.287), (-966.0), (-966.0));
      assertFalse(boolean0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
  }

  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      SystemInUtil.addInputLine("");
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(500, (-1.0E100), 500, (-420.3779948666968), 500, 88.348, 88.348, 2219.22877, 2219.22877, 500, 2219.22877);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(119, (-1338.6), (-36.613805606), 2106.750218365, 119, (-4086.0901), (-36.613805606), 13145.75809880467, (-5533.8455286), 3761.109927135618, (-36.613805606));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      String string0 = regSMO0.getRevision();
      assertEquals("8034", string0);
      assertNotNull(string0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      try { 
        regSMO0.takeStep(2, 2496, 2496, 2496, 2496);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      SelectedTag selectedTag0 = new SelectedTag(1, sGDText0.TAGS_SELECTION);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("1", selectedTag0.toString());
      
      int int0 = 2146562693;
      sGDText0.setMinWordFrequency(100);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(100.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(0, 1, 1, 1.7976931348623157E308, 0, 9.42557774, 100, 3.0, 1162.56874073, 2604.6888112528, (-1080.44608));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "{c{[");
      assertFalse(boolean0);
      
      StringKernel stringKernel0 = new StringKernel();
      assertEquals(1, StringKernel.PRUNING_LAMBDA);
      assertEquals(0, StringKernel.PRUNING_NONE);
      assertNotNull(stringKernel0);
      assertEquals("The pruning method.", stringKernel0.pruningMethodTipText());
      assertEquals("The size of the cache (a prime number).", stringKernel0.cacheSizeTipText());
      assertEquals(0.5, stringKernel0.getLambda(), 0.01);
      assertEquals("The maximum subsequence length (theta in the paper)", stringKernel0.maxSubsequenceLengthTipText());
      assertEquals("Whether to use normalization.", stringKernel0.useNormalizationTipText());
      assertFalse(stringKernel0.getChecksTurnedOff());
      assertEquals("Turns on the output of debugging information.", stringKernel0.debugTipText());
      assertEquals("Penalizes non-continuous subsequence matches, from (0,1)", stringKernel0.lambdaTipText());
      assertEquals("Turns time-consuming checks off - use with caution.", stringKernel0.checksTurnedOffTipText());
      assertEquals("The size of the internal cache (a prime number).", stringKernel0.internalCacheSizeTipText());
      assertEquals("The subsequence length.", stringKernel0.subsequenceLengthTipText());
      assertEquals(0, stringKernel0.numEvals());
      assertEquals((-1), stringKernel0.numCacheHits());
      assertEquals(200003, stringKernel0.getInternalCacheSize());
      assertEquals(9, stringKernel0.getMaxSubsequenceLength());
      assertFalse(stringKernel0.getDebug());
      assertFalse(stringKernel0.getUseNormalization());
      assertEquals(3, stringKernel0.getSubsequenceLength());
      assertEquals(250007, stringKernel0.getCacheSize());
      
      SelectedTag selectedTag0 = stringKernel0.getPruningMethod();
      assertEquals(1, StringKernel.PRUNING_LAMBDA);
      assertEquals(0, StringKernel.PRUNING_NONE);
      assertNotNull(selectedTag0);
      assertEquals("The pruning method.", stringKernel0.pruningMethodTipText());
      assertEquals("The size of the cache (a prime number).", stringKernel0.cacheSizeTipText());
      assertEquals(0.5, stringKernel0.getLambda(), 0.01);
      assertEquals("The maximum subsequence length (theta in the paper)", stringKernel0.maxSubsequenceLengthTipText());
      assertEquals("Whether to use normalization.", stringKernel0.useNormalizationTipText());
      assertFalse(stringKernel0.getChecksTurnedOff());
      assertEquals("Turns on the output of debugging information.", stringKernel0.debugTipText());
      assertEquals("Penalizes non-continuous subsequence matches, from (0,1)", stringKernel0.lambdaTipText());
      assertEquals("Turns time-consuming checks off - use with caution.", stringKernel0.checksTurnedOffTipText());
      assertEquals("The size of the internal cache (a prime number).", stringKernel0.internalCacheSizeTipText());
      assertEquals("The subsequence length.", stringKernel0.subsequenceLengthTipText());
      assertEquals(0, stringKernel0.numEvals());
      assertEquals((-1), stringKernel0.numCacheHits());
      assertEquals(200003, stringKernel0.getInternalCacheSize());
      assertEquals(9, stringKernel0.getMaxSubsequenceLength());
      assertFalse(stringKernel0.getDebug());
      assertFalse(stringKernel0.getUseNormalization());
      assertEquals(3, stringKernel0.getSubsequenceLength());
      assertEquals(250007, stringKernel0.getCacheSize());
      assertEquals("0", selectedTag0.toString());
      
      String string0 = selectedTag0.getRevision();
      assertEquals(1, StringKernel.PRUNING_LAMBDA);
      assertEquals(0, StringKernel.PRUNING_NONE);
      assertEquals("8034", string0);
      assertNotNull(string0);
      assertEquals("The pruning method.", stringKernel0.pruningMethodTipText());
      assertEquals("The size of the cache (a prime number).", stringKernel0.cacheSizeTipText());
      assertEquals(0.5, stringKernel0.getLambda(), 0.01);
      assertEquals("The maximum subsequence length (theta in the paper)", stringKernel0.maxSubsequenceLengthTipText());
      assertEquals("Whether to use normalization.", stringKernel0.useNormalizationTipText());
      assertFalse(stringKernel0.getChecksTurnedOff());
      assertEquals("Turns on the output of debugging information.", stringKernel0.debugTipText());
      assertEquals("Penalizes non-continuous subsequence matches, from (0,1)", stringKernel0.lambdaTipText());
      assertEquals("Turns time-consuming checks off - use with caution.", stringKernel0.checksTurnedOffTipText());
      assertEquals("The size of the internal cache (a prime number).", stringKernel0.internalCacheSizeTipText());
      assertEquals("The subsequence length.", stringKernel0.subsequenceLengthTipText());
      assertEquals(0, stringKernel0.numEvals());
      assertEquals((-1), stringKernel0.numCacheHits());
      assertEquals(200003, stringKernel0.getInternalCacheSize());
      assertEquals(9, stringKernel0.getMaxSubsequenceLength());
      assertFalse(stringKernel0.getDebug());
      assertFalse(stringKernel0.getUseNormalization());
      assertEquals(3, stringKernel0.getSubsequenceLength());
      assertEquals(250007, stringKernel0.getCacheSize());
      assertEquals("0", selectedTag0.toString());
      
      boolean boolean1 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1, 0.0, 0.0, 1202.8362, 0, (-2765.7641236037), 1.0, 0.0, 0.0, 1.7976931348623157E308, 2.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.setEpsilon((-2633.3454361808));
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals((-2633.3454361808), regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine((-2399), 94.9396908, (-2633.3454361808), (-2633.3454361808), (-2399), (-2633.3454361808), 1496.61, 1496.61, (-441.3048159489695), 94.9396908, (-1280.02172));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      int int0 = 1074814488;
      double double0 = (-1290.61);
      regSMO0.m_nInstances = 37;
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1074814488, 37, 4303.6, 4303.6, 501, 0.0, 4303.6, 501, 37, 1834.265976095127, 0.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine((-100), 44, 1395.7503311642058, 44, 44, (-1605), 2009.9749946282514, 1278.638055784469, (-99.0), 44, 1.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(3, (-2614), 0.05, 3595.9999996404, (-2614), (-1490.0), 1194.663, 80, (-1605), 80, 416.983);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      
      SMOset sMOset0 = new SMOset(44);
      assertNotNull(sMOset0);
      assertEquals(0, sMOset0.numElements());
      
      regSMO0.m_supportVectors = sMOset0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0, sMOset0.numElements());
      assertEquals(0, regSMO0.m_supportVectors.numElements());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      Random.setNextRandom(2099761789);
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine((-120), 44, 1395.7503311642058, 3118.56, 44, (-1605), 1202.83895448224, 1278.638055784469, (-99.0), 44, 1.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      RegSMO regSMO1 = new RegSMO();
      assertFalse(regSMO1.equals((Object)regSMO0));
      assertNotNull(regSMO1);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO1.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO1.getEpsilon(), 0.01);
      assertEquals(1, regSMO1.getSeed());
      assertEquals("Seed for random number generator.", regSMO1.seedTipText());
      assertEquals((-1), regSMO1.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO1.epsilonTipText());
      assertEquals(0, regSMO1.getKernelEvaluations());
      assertFalse(regSMO1.modelBuilt());
      assertEquals(0.001, regSMO1.getEpsilonParameter(), 0.01);
      
      double double0 = (-622.859);
      double double1 = 2.5260615183810003E-7;
      // Undeclared exception!
      try { 
        regSMO1.findOptimalPointOnLine((-2614), (-2614), 1.0E-8, 3118.56, (-2614), (-622.859), 2.5260615183810003E-7, 1.0E-6, (-2614), 0.1, 80.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("0", selectedTag0.toString());
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      assertFalse(boolean0);
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      boolean boolean1 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      int int0 = 53;
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1, 1.0E-8, 99.99999999, 0.964, 53, 0, (-1.0), 1588.93, (-1.0), 1588.93, 0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      
      String[] stringArray0 = new String[19];
      stringArray0[3] = "A.J. Smola and B. Schoelkopf";
      RegressionByDiscretization regressionByDiscretization0 = new RegressionByDiscretization();
      assertEquals(0, RegressionByDiscretization.ESTIMATOR_HISTOGRAM);
      assertEquals(1, RegressionByDiscretization.ESTIMATOR_KERNEL);
      assertEquals(2, RegressionByDiscretization.ESTIMATOR_NORMAL);
      assertNotNull(regressionByDiscretization0);
      assertEquals("If set to true, classifier may output additional info to the console.", regressionByDiscretization0.debugTipText());
      assertEquals("The density estimator to use.", regressionByDiscretization0.estimatorTypeTipText());
      assertEquals(10, regressionByDiscretization0.getNumBins());
      assertEquals("Whether to minimize absolute error.", regressionByDiscretization0.minimizeAbsoluteErrorTipText());
      assertFalse(regressionByDiscretization0.getMinimizeAbsoluteError());
      assertEquals("Whether to delete empty bins after discretization.", regressionByDiscretization0.deleteEmptyBinsTipText());
      assertFalse(regressionByDiscretization0.getUseEqualFrequency());
      assertEquals("If set to true, equal-frequency binning will be used instead of equal-width binning.", regressionByDiscretization0.useEqualFrequencyTipText());
      assertEquals("Number of bins for discretization.", regressionByDiscretization0.numBinsTipText());
      assertEquals("The base classifier to be used.", regressionByDiscretization0.classifierTipText());
      assertFalse(regressionByDiscretization0.getDeleteEmptyBins());
      assertFalse(regressionByDiscretization0.getDebug());
      
      SelectedTag selectedTag0 = regressionByDiscretization0.getEstimatorType();
      assertEquals(0, RegressionByDiscretization.ESTIMATOR_HISTOGRAM);
      assertEquals(1, RegressionByDiscretization.ESTIMATOR_KERNEL);
      assertEquals(2, RegressionByDiscretization.ESTIMATOR_NORMAL);
      assertNotNull(selectedTag0);
      assertEquals("If set to true, classifier may output additional info to the console.", regressionByDiscretization0.debugTipText());
      assertEquals("The density estimator to use.", regressionByDiscretization0.estimatorTypeTipText());
      assertEquals(10, regressionByDiscretization0.getNumBins());
      assertEquals("Whether to minimize absolute error.", regressionByDiscretization0.minimizeAbsoluteErrorTipText());
      assertFalse(regressionByDiscretization0.getMinimizeAbsoluteError());
      assertEquals("Whether to delete empty bins after discretization.", regressionByDiscretization0.deleteEmptyBinsTipText());
      assertFalse(regressionByDiscretization0.getUseEqualFrequency());
      assertEquals("If set to true, equal-frequency binning will be used instead of equal-width binning.", regressionByDiscretization0.useEqualFrequencyTipText());
      assertEquals("Number of bins for discretization.", regressionByDiscretization0.numBinsTipText());
      assertEquals("The base classifier to be used.", regressionByDiscretization0.classifierTipText());
      assertFalse(regressionByDiscretization0.getDeleteEmptyBins());
      assertFalse(regressionByDiscretization0.getDebug());
      assertEquals("0", selectedTag0.toString());
      
      String string0 = selectedTag0.getRevision();
      assertEquals(0, RegressionByDiscretization.ESTIMATOR_HISTOGRAM);
      assertEquals(1, RegressionByDiscretization.ESTIMATOR_KERNEL);
      assertEquals(2, RegressionByDiscretization.ESTIMATOR_NORMAL);
      assertEquals("8034", string0);
      assertNotNull(string0);
      assertEquals("If set to true, classifier may output additional info to the console.", regressionByDiscretization0.debugTipText());
      assertEquals("The density estimator to use.", regressionByDiscretization0.estimatorTypeTipText());
      assertEquals(10, regressionByDiscretization0.getNumBins());
      assertEquals("Whether to minimize absolute error.", regressionByDiscretization0.minimizeAbsoluteErrorTipText());
      assertFalse(regressionByDiscretization0.getMinimizeAbsoluteError());
      assertEquals("Whether to delete empty bins after discretization.", regressionByDiscretization0.deleteEmptyBinsTipText());
      assertFalse(regressionByDiscretization0.getUseEqualFrequency());
      assertEquals("If set to true, equal-frequency binning will be used instead of equal-width binning.", regressionByDiscretization0.useEqualFrequencyTipText());
      assertEquals("Number of bins for discretization.", regressionByDiscretization0.numBinsTipText());
      assertEquals("The base classifier to be used.", regressionByDiscretization0.classifierTipText());
      assertFalse(regressionByDiscretization0.getDeleteEmptyBins());
      assertFalse(regressionByDiscretization0.getDebug());
      assertEquals("0", selectedTag0.toString());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      String string1 = sGDText0.stopwordsTipText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string1);
      assertFalse(string1.equals((Object)string0));
      assertNotNull(string1);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      String string2 = regSMO0.toString();
      assertEquals("SMOreg\n\nSupport vectors:\n +       0     \n\n\n\nNumber of kernel evaluations: 0", string2);
      assertFalse(string2.equals((Object)string1));
      assertFalse(string2.equals((Object)string0));
      assertNotNull(string2);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      
      SGDText sGDText1 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText1.equals((Object)sGDText0));
      assertNotNull(sGDText1);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText1.normalizeDocLengthTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText1.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText1.lowercaseTokensTipText());
      assertEquals(500, sGDText1.getEpochs());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText1.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText1.stopwordsTipText());
      assertFalse(sGDText1.getUseStopList());
      assertEquals("The learning rate.", sGDText1.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText1.periodicPruningTipText());
      assertFalse(sGDText1.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText1.minWordFrequencyTipText());
      assertEquals(3.0, sGDText1.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText1.debugTipText());
      assertFalse(sGDText1.getDebug());
      assertEquals(0, sGDText1.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText1.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText1.seedTipText());
      assertEquals(2.0, sGDText1.getLNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText1.normTipText());
      assertFalse(sGDText1.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", sGDText1.LNormTipText());
      assertEquals(1, sGDText1.getSeed());
      assertEquals(0.01, sGDText1.getLearningRate(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText1.useWordFrequenciesTipText());
      assertFalse(sGDText1.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText1.lambdaTipText());
      assertEquals(1.0E-4, sGDText1.getLambda(), 0.01);
      assertFalse(sGDText1.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText1.lossFunctionTipText());
      assertEquals(1.0, sGDText1.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText1.outputProbsForSVMTipText());
      
      String string3 = sGDText1.lambdaTipText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The regularization constant. (default = 0.0001)", string3);
      assertNotSame(sGDText1, sGDText0);
      assertFalse(sGDText1.equals((Object)sGDText0));
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string0));
      assertFalse(string3.equals((Object)string2));
      assertNotNull(string3);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText1.normalizeDocLengthTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText1.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText1.lowercaseTokensTipText());
      assertEquals(500, sGDText1.getEpochs());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText1.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText1.stopwordsTipText());
      assertFalse(sGDText1.getUseStopList());
      assertEquals("The learning rate.", sGDText1.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText1.periodicPruningTipText());
      assertFalse(sGDText1.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText1.minWordFrequencyTipText());
      assertEquals(3.0, sGDText1.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText1.debugTipText());
      assertFalse(sGDText1.getDebug());
      assertEquals(0, sGDText1.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText1.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText1.seedTipText());
      assertEquals(2.0, sGDText1.getLNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText1.normTipText());
      assertFalse(sGDText1.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", sGDText1.LNormTipText());
      assertEquals(1, sGDText1.getSeed());
      assertEquals(0.01, sGDText1.getLearningRate(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText1.useWordFrequenciesTipText());
      assertFalse(sGDText1.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText1.lambdaTipText());
      assertEquals(1.0E-4, sGDText1.getLambda(), 0.01);
      assertFalse(sGDText1.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText1.lossFunctionTipText());
      assertEquals(1.0, sGDText1.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText1.outputProbsForSVMTipText());
      
      sGDText0.setLambda(0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotSame(sGDText0, sGDText1);
      assertFalse(sGDText0.equals((Object)sGDText1));
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(0.0, sGDText0.getLambda(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(16, (-1800.8174), 2, 1685.994, 1, 1685.994, 1.3, 824.3, 0, 1, 1685.994);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(62, 1909.3, 62, 62, (-872), 2736.4325076, 566.85776114, 0.1, 50.0, 566.85776114, 50.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      
      SystemInUtil.addInputLine((String) null);
      sGDText0.setOutputProbsForSVM(true);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertTrue(sGDText0.getOutputProbsForSVM());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      
      String string0 = regSMO0.getRevision();
      assertEquals("8034", string0);
      assertNotNull(string0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      double double0 = (-6.056428078139942);
      int int0 = 115;
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(115, 0, (-6.056428078139942), 0, 115, 115, 0, 1.7976931348623157E308, (-1), 3753.9027, 1.0E-10);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      SMOset sMOset0 = new SMOset(16);
      assertNotNull(sMOset0);
      assertEquals(0, sMOset0.numElements());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(selectedTag0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("0", selectedTag0.toString());
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      sMOset0.printElements();
      assertEquals(0, sMOset0.numElements());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(0, 0.0, 1.7976931346825464E308, 1037.7983562891, 432, 3743.749291322762, 1199.403837, 16, 0.05, 0.05, (-152.910197256));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      
      SelectedTag selectedTag0 = new SelectedTag(1, sGDText0.TAGS_SELECTION);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(selectedTag0);
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("1", selectedTag0.toString());
      
      double[] doubleArray0 = new double[6];
      doubleArray0[1] = (double) 1;
      sGDText0.setMinWordFrequency((-0.002));
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals((-0.002), sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(7, 100.0, 2500, 2500, 2500, 1.0E-12, 6.283185307179586, 2.147483647E9, 6.283185307179586, 6.283185307179586, 2146562693);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      
      SelectedTag selectedTag0 = new SelectedTag(1, sGDText0.TAGS_SELECTION);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("1", selectedTag0.toString());
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "{c{[");
      assertFalse(boolean0);
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      String string0 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string0);
      assertNotNull(string0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("1", selectedTag0.toString());
      
      BestFirst bestFirst0 = new BestFirst();
      assertNotNull(bestFirst0);
      assertEquals("Set the start point for the search. This is specified as a comma seperated list off attribute indexes starting at 1. It can include ranges. Eg. 1,2,5-9,17.", bestFirst0.startSetTipText());
      assertEquals(5, bestFirst0.getSearchTermination());
      assertEquals("BestFirst:\n\nSearches the space of attribute subsets by greedy hillclimbing augmented with a backtracking facility. Setting the number of consecutive non-improving nodes allowed controls the level of backtracking done. Best first may start with the empty set of attributes and search forward, or start with the full set of attributes and search backward, or start at any point and search in both directions (by considering all possible single attribute additions and deletions at a given point).\n", bestFirst0.globalInfo());
      assertEquals(1, bestFirst0.getLookupCacheSize());
      assertEquals("Set the direction of the search.", bestFirst0.directionTipText());
      assertEquals("Set the amount of backtracking. Specify the number of ", bestFirst0.searchTerminationTipText());
      assertEquals("Set the maximum size of the lookup cache of evaluated subsets. This is expressed as a multiplier of the number of attributes in the data set. (default = 1).", bestFirst0.lookupCacheSizeTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1, 1.0, 1, 546.0087986, 0, (-1.0), 0, 0, 0, 45.667205599975, 2275.888089381);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test30()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      String string0 = regSMO0.globalInfo();
      assertEquals("Implementation of SMO for support vector regression as described in :\n\nA.J. Smola, B. Schoelkopf (1998). A tutorial on support vector regression.", string0);
      assertNotNull(string0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.setEpsilonParameter((-3411.59366326));
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals((-3411.59366326), regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(34, 0.0, 2068889365, 2068889365, (-802), (-3411.59366326), 0.0, 3076.34, 789.399562, 3595.9999996404, 1955.71914);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test31()  throws Throwable  {
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      boolean boolean1 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "aS:'<lpMEZ#&lOZmd");
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      boolean boolean2 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "aS:'<lpMEZ#&lOZmd");
      assertFalse(boolean2);
      assertTrue(boolean2 == boolean1);
      assertTrue(boolean2 == boolean0);
      
      regSMO0.m_nCacheHits = (-2615);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-2615), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      regSMO0.m_nSeed = (-2615);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-2615), regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals((-2615), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-2615), regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals((-2615), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      Random.setNextRandom(2146177964);
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(90, 90, 90, 90, (-1586), 90, (-1586), 3118.56, 90, 90, (-1586));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test32()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(selectedTag0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[6];
      doubleArray0[1] = (double) 1;
      regSMO0.m_b = (-0.002);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      sGDText0.setMinWordFrequency(0.4957210968140902);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(0.4957210968140902, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      String[] stringArray0 = sGDText0.getOptions();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(stringArray0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(0.4957210968140902, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1, 100, 1.0, 1.0, 1, (-1344.385082649016), 492.3402165, 100, (-0.002), 100, 0.4957210968140902);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test33()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      SelectedTag selectedTag0 = new SelectedTag(1, sGDText0.TAGS_SELECTION);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("1", selectedTag0.toString());
      
      sGDText0.setLearningRate(2072.0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2072.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      double[] doubleArray0 = new double[6];
      String string0 = sGDText0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string0);
      assertNotNull(string0);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2072.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      doubleArray0[1] = (double) 1;
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      doubleArray0[4] = (double) 0;
      String string1 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string1);
      assertTrue(string1.equals((Object)string0));
      assertNotNull(string1);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2072.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("1", selectedTag0.toString());
      
      String string2 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string2);
      assertTrue(string2.equals((Object)string0));
      assertTrue(string2.equals((Object)string1));
      assertNotNull(string2);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2072.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("1", selectedTag0.toString());
      
      regSMO0.m_alpha = doubleArray0;
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      String string3 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string3);
      assertTrue(string3.equals((Object)string2));
      assertTrue(string3.equals((Object)string0));
      assertTrue(string3.equals((Object)string1));
      assertNotNull(string3);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2072.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("1", selectedTag0.toString());
      
      String string4 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string4);
      assertTrue(string4.equals((Object)string3));
      assertTrue(string4.equals((Object)string0));
      assertTrue(string4.equals((Object)string1));
      assertTrue(string4.equals((Object)string2));
      assertNotNull(string4);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2072.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("1", selectedTag0.toString());
      
      String string5 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string5);
      assertTrue(string5.equals((Object)string3));
      assertTrue(string5.equals((Object)string0));
      assertTrue(string5.equals((Object)string1));
      assertTrue(string5.equals((Object)string4));
      assertTrue(string5.equals((Object)string2));
      assertNotNull(string5);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2072.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("1", selectedTag0.toString());
      
      String string6 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string6);
      assertTrue(string6.equals((Object)string4));
      assertTrue(string6.equals((Object)string2));
      assertTrue(string6.equals((Object)string0));
      assertTrue(string6.equals((Object)string1));
      assertTrue(string6.equals((Object)string3));
      assertTrue(string6.equals((Object)string5));
      assertNotNull(string6);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2072.0, sGDText0.getLearningRate(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("1", selectedTag0.toString());
      
      BestFirst bestFirst0 = new BestFirst();
      assertNotNull(bestFirst0);
      assertEquals("Set the maximum size of the lookup cache of evaluated subsets. This is expressed as a multiplier of the number of attributes in the data set. (default = 1).", bestFirst0.lookupCacheSizeTipText());
      assertEquals("Set the amount of backtracking. Specify the number of ", bestFirst0.searchTerminationTipText());
      assertEquals(1, bestFirst0.getLookupCacheSize());
      assertEquals("Set the direction of the search.", bestFirst0.directionTipText());
      assertEquals(5, bestFirst0.getSearchTermination());
      assertEquals("Set the start point for the search. This is specified as a comma seperated list off attribute indexes starting at 1. It can include ranges. Eg. 1,2,5-9,17.", bestFirst0.startSetTipText());
      assertEquals("BestFirst:\n\nSearches the space of attribute subsets by greedy hillclimbing augmented with a backtracking facility. Setting the number of consecutive non-improving nodes allowed controls the level of backtracking done. Best first may start with the empty set of attributes and search forward, or start with the full set of attributes and search backward, or start at any point and search in both directions (by considering all possible single attribute additions and deletions at a given point).\n", bestFirst0.globalInfo());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine((-1545), 0.0, (-4.57826535), 4384.157, (-1545), (-2711.8), 1.0E-8, (-3675.0), 1.0E-12, 4.69E-8, (-4.57826535));
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // -1545
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test34()  throws Throwable  {
      Random.setNextRandom(2492);
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(2492, 2418.89, (-4630.0), 5.5, 2492, 5.5, (-1144.6410908117687), 54.916688402, 1.0E-10, 1843.4768610265162, 2492);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test35()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      
      sGDText0.reset();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      
      Capabilities capabilities0 = sGDText0.getCapabilities();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(capabilities0);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      double[] doubleArray0 = new double[8];
      regSMO0.m_alphaStar = doubleArray0;
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      doubleArray0[0] = (double) 1;
      regSMO0.m_alpha = doubleArray0;
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      boolean boolean0 = regSMO0.m_bModelBuilt;
      assertFalse(boolean0);
      
      regSMO0.m_alpha2Star = (double) 1;
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      int int0 = regSMO0.secondChoiceHeuristic(1);
      assertEquals((-1), int0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.setEpsilon(1.0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1, (-441.3048159931), 0, 25.6188988, 0, 499.217099, 1, (-441.3048159931), 499.217099, 1, 0.9999999999);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test36()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      
      SelectedTag selectedTag0 = new SelectedTag(1, sGDText0.TAGS_SELECTION);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(selectedTag0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("1", selectedTag0.toString());
      
      double[] doubleArray0 = new double[6];
      doubleArray0[1] = (double) 1;
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      doubleArray0[5] = (double) 0;
      String string0 = selectedTag0.getRevision();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("8034", string0);
      assertNotNull(string0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("1", selectedTag0.toString());
      
      regSMO0.m_alpha = doubleArray0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      String string1 = selectedTag0.getRevision();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("8034", string1);
      assertTrue(string1.equals((Object)string0));
      assertNotNull(string1);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("1", selectedTag0.toString());
      
      String string2 = selectedTag0.getRevision();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("8034", string2);
      assertTrue(string2.equals((Object)string1));
      assertTrue(string2.equals((Object)string0));
      assertNotNull(string2);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("1", selectedTag0.toString());
      
      String string3 = sGDText0.lossFunctionTipText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", string3);
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string2));
      assertFalse(string3.equals((Object)string0));
      assertNotNull(string3);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      
      String string4 = regSMO0.toString();
      assertEquals("SMOreg\n\nSupport vectors:\n +       0     \n\n\n\nNumber of kernel evaluations: 0", string4);
      assertFalse(string4.equals((Object)string1));
      assertFalse(string4.equals((Object)string3));
      assertFalse(string4.equals((Object)string0));
      assertFalse(string4.equals((Object)string2));
      assertNotNull(string4);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      String string5 = regSMO0.epsilonTipText();
      assertEquals("The epsilon for round-off error (shouldn't be changed).", string5);
      assertFalse(string5.equals((Object)string4));
      assertFalse(string5.equals((Object)string1));
      assertFalse(string5.equals((Object)string3));
      assertFalse(string5.equals((Object)string2));
      assertFalse(string5.equals((Object)string0));
      assertNotNull(string5);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      Random.setNextRandom(0);
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(0, (-1096.0), 0.2, 1.0, (-2142002147), (-2142002147), 0, 1, 1619.494, 1619.494, 3486);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test37()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[21];
      double[] doubleArray1 = new double[7];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) 1;
      doubleArray1[4] = 1.0;
      regSMO0.m_nEvals = 1;
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      doubleArray1[5] = (double) 1;
      regSMO0.m_alphaStar = doubleArray1;
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      regSMO0.m_alpha = doubleArray0;
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1, 1, 1.0, 1.0, 0, 1.7976931346825464E308, 0.0, 0, 0, 839.68037507, 1.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test38()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      int int0 = LWL.TRICUBE;
      assertEquals(2, int0);
      
      String string0 = regSMO0.globalInfo();
      assertEquals("Implementation of SMO for support vector regression as described in :\n\nA.J. Smola, B. Schoelkopf (1998). A tutorial on support vector regression.", string0);
      assertNotNull(string0);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      String[] stringArray0 = sGDText0.getOptions();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(stringArray0);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      regSMO0.setOptions(stringArray0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0.01, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.0, regSMO0.getEpsilon(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      
      try { 
        regSMO0.optimize();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test39()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      boolean boolean0 = FileSystemHandling.setPermissions((EvoSuiteFile) null, false, false, false);
      assertFalse(boolean0);
      
      boolean boolean1 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "aS:'<lpMEZ#&lOZmd");
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      regSMO0.m_nCacheHits = (-2615);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-2615), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      regSMO0.m_nSeed = (-2615);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-2615), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-2615), regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-2615), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-2615), regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      regSMO0.setEpsilonParameter(1625.8869639918387);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(1625.8869639918387, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-2615), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-2615), regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine((-2615), 6.0, 1625.8869639918387, 1473.69, 90, 1.0E-10, (-2615), 90, 5.70001, 1651.59112172039, 1625.8869639918387);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test40()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      SMOset sMOset0 = new SMOset(2);
      assertNotNull(sMOset0);
      assertEquals(0, sMOset0.numElements());
      
      regSMO0.m_supportVectors = sMOset0;
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, sMOset0.numElements());
      assertEquals(0, regSMO0.m_supportVectors.numElements());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (double) 1;
      doubleArray0[4] = 0.0;
      regSMO0.m_alphaStar = doubleArray0;
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      doubleArray0[0] = (double) 0;
      regSMO0.m_alpha = regSMO0.m_alphaStar;
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      boolean boolean0 = regSMO0.findOptimalPointOnLine(1, 1, 1, 0.0, 0, 1, 0.0, 26.32366263351166, 26.32366263351166, 47.7129310894, 1.0E-8);
      assertTrue(boolean0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      String[] stringArray0 = regSMO0.getOptions();
      assertNotNull(stringArray0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      
      try { 
        regSMO0.wrapUp();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegOptimizer", e);
      }
  }

  @Test(timeout = 4000)
  public void test41()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      
      double[] doubleArray0 = new double[6];
      doubleArray0[1] = (double) 1;
      double[] doubleArray1 = new double[7];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) 1;
      doubleArray1[2] = (double) 0;
      doubleArray1[4] = 1.0;
      doubleArray1[5] = (double) 1;
      regSMO0.m_alphaStar = doubleArray1;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      doubleArray1[0] = (double) 1;
      doubleArray0[5] = (double) 0;
      regSMO0.m_alpha = doubleArray0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1, 1, 0.0, 20.48305618170374, 0, 3.5959999996404003E-7, 1, 4.69E-8, 4.69E-8, 1.0E-10, 1.0E-8);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test42()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      SMOset sMOset0 = new SMOset(2);
      assertNotNull(sMOset0);
      assertEquals(0, sMOset0.numElements());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (double) 1;
      doubleArray0[4] = 0.0;
      regSMO0.m_alphaStar = doubleArray0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      doubleArray0[0] = (double) 0;
      regSMO0.m_alpha = regSMO0.m_alphaStar;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      double double0 = 4.69E-8;
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(0, 26.32366263351166, 4.69E-8, 948.9706745, 45, 2.619704055917625, 0.1, 10.050627363527312, 2.619704055917625, 0.1, 0.05);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 45
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test43()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("0", selectedTag0.toString());
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "A tutorial on support vector regression");
      assertFalse(boolean0);
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      
      SGDText sGDText1 = (SGDText)AbstractClassifier.makeCopy(sGDText0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotSame(sGDText0, sGDText1);
      assertNotSame(sGDText1, sGDText0);
      assertFalse(sGDText1.equals((Object)sGDText0));
      assertNotNull(sGDText1);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText1.globalInfo());
      assertFalse(sGDText1.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText1.normTipText());
      assertEquals("The random number seed to be used.", sGDText1.seedTipText());
      assertEquals(1.0, sGDText1.getNorm(), 0.01);
      assertEquals("The learning rate.", sGDText1.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText1.tokenizerTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText1.lossFunctionTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText1.periodicPruningTipText());
      assertEquals(0.01, sGDText1.getLearningRate(), 0.01);
      assertEquals(1, sGDText1.getSeed());
      assertEquals(1.0E-4, sGDText1.getLambda(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText1.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText1.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText1.normalizeDocLengthTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText1.epochsTipText());
      assertEquals(2.0, sGDText1.getLNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText1.outputProbsForSVMTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText1.debugTipText());
      assertFalse(sGDText1.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText1.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText1.minWordFrequencyTipText());
      assertFalse(sGDText1.getUseStopList());
      assertFalse(sGDText1.getOutputProbsForSVM());
      assertFalse(sGDText1.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText1.lambdaTipText());
      assertEquals(0, sGDText1.getPeriodicPruning());
      assertEquals(3.0, sGDText1.getMinWordFrequency(), 0.01);
      assertFalse(sGDText1.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText1.stopwordsTipText());
      assertEquals(500, sGDText1.getEpochs());
      
      String string0 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string0);
      assertNotSame(sGDText0, sGDText1);
      assertFalse(sGDText0.equals((Object)sGDText1));
      assertNotNull(string0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("0", selectedTag0.toString());
      
      sGDText0.setOutputProbsForSVM(false);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotSame(sGDText0, sGDText1);
      assertFalse(sGDText0.equals((Object)sGDText1));
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      
      boolean boolean1 = FileSystemHandling.setPermissions((EvoSuiteFile) null, true, true, false);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      MockFile mockFile0 = (MockFile)sGDText0.getStopwords();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotSame(sGDText0, sGDText1);
      assertFalse(sGDText0.equals((Object)sGDText1));
      assertNotNull(mockFile0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertTrue(mockFile0.canWrite());
      assertTrue(mockFile0.canExecute());
      assertTrue(mockFile0.canRead());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals("/mnt/gaiagpfs/users/homedirs/apanichella/Evosuite_performance/Dataset/gordon_script_sum/projects", mockFile0.getParent());
      assertTrue(mockFile0.exists());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals("9_weka", mockFile0.getName());
      assertFalse(mockFile0.isFile());
      assertEquals(0L, mockFile0.length());
      assertTrue(mockFile0.isAbsolute());
      assertEquals("/mnt/gaiagpfs/users/homedirs/apanichella/Evosuite_performance/Dataset/gordon_script_sum/projects/9_weka", mockFile0.toString());
      assertTrue(mockFile0.isDirectory());
      
      sGDText0.setStopwords(mockFile0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotSame(sGDText0, sGDText1);
      assertFalse(sGDText0.equals((Object)sGDText1));
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertTrue(mockFile0.canWrite());
      assertTrue(mockFile0.canExecute());
      assertTrue(mockFile0.canRead());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals("/mnt/gaiagpfs/users/homedirs/apanichella/Evosuite_performance/Dataset/gordon_script_sum/projects", mockFile0.getParent());
      assertTrue(mockFile0.exists());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals("9_weka", mockFile0.getName());
      assertFalse(mockFile0.isFile());
      assertEquals(0L, mockFile0.length());
      assertTrue(mockFile0.isAbsolute());
      assertEquals("/mnt/gaiagpfs/users/homedirs/apanichella/Evosuite_performance/Dataset/gordon_script_sum/projects/9_weka", mockFile0.toString());
      assertTrue(mockFile0.isDirectory());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(2, 0.0, 0.0, 1594.31962493, 0, 0.0, 1594.31962493, 1.0E-8, 3.5959999996404003E-7, 3.5959999996404003E-7, 1.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test44()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      
      double[] doubleArray0 = new double[12];
      doubleArray0[1] = (double) 1;
      double[] doubleArray1 = new double[7];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) 1;
      doubleArray1[2] = (double) 1;
      doubleArray1[4] = 1.7976931348623157E308;
      doubleArray1[5] = (double) 1;
      regSMO0.m_alphaStar = doubleArray1;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      
      doubleArray1[0] = (double) 1;
      doubleArray0[5] = (double) 0;
      regSMO0.m_alpha = doubleArray0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(1, 1, 1.0, (-1.0), 0, 3.5959999996404003E-7, 1, 4.69E-8, 4.69E-8, 1.0E-10, 1.0E-8);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test45()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(selectedTag0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("0", selectedTag0.toString());
      
      sGDText0.setMinWordFrequency((-2873.50997343663));
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals((-2873.50997343663), sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      
      int int0 = 2697;
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(2697, 0.002, 275.11339889440023, 2697, 1642, 0, 0, (-966.0), 0, 2146562693, (-1497.4723));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test46()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.m_nInstances = 222;
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (double) 222;
      doubleArray0[1] = 46.667205599975;
      try { 
        regSMO0.optimize();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test47()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      double[] doubleArray0 = new double[14];
      double[] doubleArray1 = new double[7];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      regSMO0.m_alphaStar = doubleArray1;
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      regSMO0.m_alpha = doubleArray1;
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      SMOreg sMOreg0 = regSMO0.m_SVM;
      assertNull(sMOreg0);
      
      int int0 = regSMO0.secondChoiceHeuristic(75);
      assertEquals((-1), int0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      try { 
        regSMO0.wrapUp();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegOptimizer", e);
      }
  }

  @Test(timeout = 4000)
  public void test48()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      
      double[] doubleArray0 = new double[21];
      double[] doubleArray1 = new double[7];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) 1;
      regSMO0.m_alphaStar = doubleArray1;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      regSMO0.m_C = 22.0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      regSMO0.m_alpha = doubleArray0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      regSMO0.m_target = doubleArray1;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      int int0 = regSMO0.secondChoiceHeuristic(0);
      assertEquals(0, int0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      try { 
        regSMO0.optimize();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test49()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      sGDText0.reset();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      double[] doubleArray0 = new double[8];
      doubleArray0[4] = (double) 1;
      Enumeration<Option> enumeration0 = sGDText0.listOptions();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(enumeration0);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      regSMO0.m_error = doubleArray0;
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      regSMO0.m_alphaStar = doubleArray0;
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      doubleArray0[0] = (double) 1;
      regSMO0.m_alpha = doubleArray0;
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      boolean boolean0 = regSMO0.m_bModelBuilt;
      assertFalse(boolean0);
      
      int int0 = regSMO0.secondChoiceHeuristic(1);
      assertEquals((-1), int0);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      try { 
        regSMO0.optimize();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test50()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 1.0E-12;
      regSMO0.m_alpha = doubleArray0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      
      int int0 = regSMO0.secondChoiceHeuristic((-3194));
      assertEquals(0, int0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
  }

  @Test(timeout = 4000)
  public void test51()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      regSMO0.m_nSeed = 1285;
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1285, regSMO0.getSeed());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (double) 1285;
      regSMO0.m_alpha = doubleArray0;
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1285, regSMO0.getSeed());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1, sGDText0.getSeed());
      
      // Undeclared exception!
      try { 
        regSMO0.secondChoiceHeuristic(1);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test52()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(technicalInformation0.hasAdditional());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[21];
      doubleArray0[1] = (double) 1;
      double[] doubleArray1 = new double[7];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) 1;
      doubleArray1[5] = (double) 1;
      regSMO0.m_alphaStar = doubleArray1;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      doubleArray1[0] = (double) 0;
      int[] intArray0 = new int[5];
      SGDText sGDText1 = (SGDText)AbstractClassifier.makeCopy(sGDText0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotSame(sGDText0, sGDText1);
      assertNotSame(sGDText1, sGDText0);
      assertFalse(sGDText1.equals((Object)sGDText0));
      assertNotNull(sGDText1);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText1.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText1.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText1.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText1.stemmerTipText());
      assertEquals(500, sGDText1.getEpochs());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText1.useStopListTipText());
      assertEquals("The learning rate.", sGDText1.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText1.periodicPruningTipText());
      assertFalse(sGDText1.getLowercaseTokens());
      assertEquals("The random number seed to be used.", sGDText1.seedTipText());
      assertFalse(sGDText1.getDebug());
      assertEquals(0, sGDText1.getPeriodicPruning());
      assertEquals(3.0, sGDText1.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText1.minWordFrequencyTipText());
      assertEquals(2.0, sGDText1.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText1.debugTipText());
      assertFalse(sGDText1.getUseStopList());
      assertEquals("The norm of the instances after normalization.", sGDText1.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText1.LNormTipText());
      assertEquals(1, sGDText1.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText1.tokenizerTipText());
      assertFalse(sGDText1.getNormalizeDocLength());
      assertFalse(sGDText1.getOutputProbsForSVM());
      assertEquals(0.01, sGDText1.getLearningRate(), 0.01);
      assertEquals("The regularization constant. (default = 0.0001)", sGDText1.lambdaTipText());
      assertFalse(sGDText1.getUseWordFrequencies());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText1.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText1.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText1.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText1.lossFunctionTipText());
      assertEquals(1.0, sGDText1.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText1.globalInfo());
      
      intArray0[1] = 1;
      intArray0[2] = 0;
      intArray0[3] = 1;
      intArray0[4] = 1;
      regSMO0.m_sparseIndices = intArray0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      regSMO0.m_alpha = doubleArray0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      
      String string0 = selectedTag0.getRevision();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("8034", string0);
      assertNotSame(sGDText0, sGDText1);
      assertFalse(sGDText0.equals((Object)sGDText1));
      assertNotNull(string0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("0", selectedTag0.toString());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(0, 0, 1, 3.5959999996404003E-7, 0, 0.0, 2418.89, 0.0, 0.0, 1.9193594908734122, 0.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test53()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      SMOset sMOset0 = new SMOset(2);
      assertNotNull(sMOset0);
      assertEquals(0, sMOset0.numElements());
      
      regSMO0.m_supportVectors = sMOset0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0, sMOset0.numElements());
      assertEquals(0, regSMO0.m_supportVectors.numElements());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1, sGDText0.getSeed());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[7];
      doubleArray0[4] = (double) 1;
      regSMO0.m_alphaStar = doubleArray0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      doubleArray0[0] = (double) 0;
      regSMO0.m_alpha = regSMO0.m_alphaStar;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      SparseInstance sparseInstance0 = new SparseInstance(0, regSMO0.m_alpha);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(7, sparseInstance0.numAttributes());
      assertEquals(1, sparseInstance0.numValues());
      assertEquals(0.0, sparseInstance0.weight(), 0.01);
      
      boolean boolean1 = sparseInstance0.isMissing(1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(7, sparseInstance0.numAttributes());
      assertEquals(1, sparseInstance0.numValues());
      assertEquals(0.0, sparseInstance0.weight(), 0.01);
      
      boolean boolean2 = regSMO0.findOptimalPointOnLine(0, 2.0, 6, (-6.056428078745585), 0, 0.0, 6, 0.0, 0.0, 1, 15.0);
      assertTrue(boolean2);
      assertFalse(boolean2 == boolean0);
      assertFalse(boolean2 == boolean1);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      String string0 = regSMO0.epsilonTipText();
      assertEquals("The epsilon for round-off error (shouldn't be changed).", string0);
      assertNotNull(string0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      System.setCurrentTimeMillis((-3064L));
  }

  @Test(timeout = 4000)
  public void test54()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      SMOset sMOset0 = new SMOset(2);
      assertNotNull(sMOset0);
      assertEquals(0, sMOset0.numElements());
      
      regSMO0.m_supportVectors = sMOset0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, sMOset0.numElements());
      assertEquals(0, regSMO0.m_supportVectors.numElements());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(selectedTag0);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[7];
      doubleArray0[4] = (double) 1;
      regSMO0.m_alphaStar = doubleArray0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      doubleArray0[0] = (double) 0;
      regSMO0.m_alpha = regSMO0.m_alphaStar;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      SparseInstance sparseInstance0 = new SparseInstance(0, regSMO0.m_alpha);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(7, sparseInstance0.numAttributes());
      assertEquals(1, sparseInstance0.numValues());
      assertEquals(0.0, sparseInstance0.weight(), 0.01);
      
      boolean boolean1 = sparseInstance0.isMissing(1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(7, sparseInstance0.numAttributes());
      assertEquals(1, sparseInstance0.numValues());
      assertEquals(0.0, sparseInstance0.weight(), 0.01);
      
      boolean boolean2 = regSMO0.findOptimalPointOnLine(0, 2.0, 6, (-6.056428078745585), 0, (-244.4515406990358), 6, (-244.4515406990358), (-244.4515406990358), 1, 15.0);
      assertTrue(boolean2);
      assertFalse(boolean2 == boolean1);
      assertFalse(boolean2 == boolean0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      
      String string0 = regSMO0.epsilonTipText();
      assertEquals("The epsilon for round-off error (shouldn't be changed).", string0);
      assertNotNull(string0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
  }

  @Test(timeout = 4000)
  public void test55()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(1, regSMO0.getSeed());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      
      SelectedTag selectedTag0 = new SelectedTag(1, sGDText0.TAGS_SELECTION);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(selectedTag0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("1", selectedTag0.toString());
      
      double[] doubleArray0 = new double[6];
      doubleArray0[1] = (double) 1;
      regSMO0.m_b = (-0.002);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(1, regSMO0.getSeed());
      
      sGDText0.setMinWordFrequency(0.4957210968140902);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(0.4957210968140902, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      
      String[] stringArray0 = sGDText0.getOptions();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(stringArray0);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(0.4957210968140902, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      sGDText0.setLambda(2027.0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(0.4957210968140902, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(2027.0, sGDText0.getLambda(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(0, (-1262.83), 1.0E-12, 1, 84, (-1344.385082649016), 0.4957210968140902, 5262.004, (-0.002), 1051.62620431, (-722.236159168419));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test56()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      SMOset sMOset0 = new SMOset(16);
      assertNotNull(sMOset0);
      assertEquals(0, sMOset0.numElements());
      
      regSMO0.m_supportVectors = sMOset0;
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0, sMOset0.numElements());
      assertEquals(0, regSMO0.m_supportVectors.numElements());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(selectedTag0);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[18];
      double[] doubleArray1 = new double[9];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) 1;
      doubleArray1[4] = 1.0;
      regSMO0.m_nEvals = 1;
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      doubleArray1[5] = (double) 1;
      regSMO0.m_alphaStar = doubleArray1;
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      regSMO0.m_alpha = doubleArray0;
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      sMOset0.printElements();
      assertEquals(0, sMOset0.numElements());
      
      boolean boolean0 = FileSystemHandling.createFolder((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      int int0 = 0;
      boolean boolean1 = regSMO0.findOptimalPointOnLine(0, 0.0, (-2136.874886893546), 1.0, 0, (-1.0), 4.69E-8, 1.0, 4.69E-8, 10.050627363527312, 2485.94227207342);
      assertTrue(boolean1);
      assertFalse(boolean1 == boolean0);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      
      Random.setNextRandom((-275));
      try { 
        regSMO0.wrapUp();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegOptimizer", e);
      }
  }

  @Test(timeout = 4000)
  public void test57()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      
      sGDText0.reset();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      
      double[] doubleArray0 = new double[8];
      Enumeration<Option> enumeration0 = sGDText0.listOptions();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(enumeration0);
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      
      regSMO0.m_error = doubleArray0;
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      regSMO0.m_alphaStar = doubleArray0;
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      doubleArray0[0] = (double) 1;
      regSMO0.m_alpha = doubleArray0;
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      boolean boolean0 = regSMO0.m_bModelBuilt;
      assertFalse(boolean0);
      
      int int0 = regSMO0.secondChoiceHeuristic(1);
      assertEquals((-1), int0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      regSMO0.setEpsilon(1.0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      boolean boolean1 = regSMO0.findOptimalPointOnLine(1, 1.0, 0, 25.6188988, 0, 2.0, 1, (-441.3048159931), 499.217099, 1, 0.9999999999);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      Instances instances0 = regSMO0.m_data;
      assertNull(instances0);
      
      try { 
        regSMO0.buildClassifier((Instances) null);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // SVM not initialized in optimizer. Use RegOptimizer.setSVMReg()
         //
         verifyException("weka.classifiers.functions.supportVector.RegOptimizer", e);
      }
  }

  @Test(timeout = 4000)
  public void test58()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      
      SelectedTag selectedTag0 = sGDText0.getLossFunction();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(selectedTag0);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("0", selectedTag0.toString());
      
      double[] doubleArray0 = new double[21];
      doubleArray0[1] = (double) 1;
      double[] doubleArray1 = new double[7];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      doubleArray1[0] = (double) 1;
      doubleArray1[4] = 0.0;
      doubleArray1[5] = (double) 1;
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "{c{[");
      assertFalse(boolean0);
      
      regSMO0.m_alphaStar = doubleArray1;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      
      Enumeration enumeration0 = regSMO0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      
      doubleArray1[0] = (double) 0;
      int[] intArray0 = new int[5];
      SGDText sGDText1 = (SGDText)AbstractClassifier.makeCopy(sGDText0);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotSame(sGDText0, sGDText1);
      assertNotSame(sGDText1, sGDText0);
      assertFalse(sGDText1.equals((Object)sGDText0));
      assertNotNull(sGDText1);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText1.useStopListTipText());
      assertEquals(3.0, sGDText1.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText1.debugTipText());
      assertFalse(sGDText1.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText1.lambdaTipText());
      assertEquals(0, sGDText1.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText1.stopwordsTipText());
      assertEquals(500, sGDText1.getEpochs());
      assertEquals(0.01, sGDText1.getLearningRate(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText1.globalInfo());
      assertEquals(1.0, sGDText1.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText1.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText1.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText1.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText1.useWordFrequenciesTipText());
      assertFalse(sGDText1.getOutputProbsForSVM());
      assertFalse(sGDText1.getUseStopList());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText1.lossFunctionTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText1.LNormTipText());
      assertEquals(1.0E-4, sGDText1.getLambda(), 0.01);
      assertFalse(sGDText1.getLowercaseTokens());
      assertEquals(1, sGDText1.getSeed());
      assertFalse(sGDText1.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", sGDText1.normTipText());
      assertEquals("The learning rate.", sGDText1.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText1.periodicPruningTipText());
      assertEquals("The random number seed to be used.", sGDText1.seedTipText());
      assertEquals(2.0, sGDText1.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText1.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText1.stemmerTipText());
      assertFalse(sGDText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText1.tokenizerTipText());
      
      intArray0[1] = 1;
      intArray0[2] = 0;
      intArray0[3] = 1;
      intArray0[4] = 0;
      regSMO0.m_sparseIndices = intArray0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      
      regSMO0.m_alpha = doubleArray0;
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertFalse(regSMO0.modelBuilt());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals((-1), regSMO0.getCacheHits());
      
      String string0 = selectedTag0.getRevision();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("8034", string0);
      assertNotSame(sGDText0, sGDText1);
      assertFalse(sGDText0.equals((Object)sGDText1));
      assertNotNull(string0);
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("0", selectedTag0.toString());
      
      sGDText0.setOutputProbsForSVM(false);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotSame(sGDText0, sGDText1);
      assertFalse(sGDText0.equals((Object)sGDText1));
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      
      boolean boolean1 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "8034");
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      // Undeclared exception!
      try { 
        regSMO0.findOptimalPointOnLine(0, 0, 0, 3.5959999996404003E-7, 0, 0.0, 2418.89, 0.0, 0.0, 1.9193594908734122, 0.0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test59()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (double) 2068889365;
      regSMO0.m_nCacheHits = 2068889365;
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(2068889365, regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.m_alpha1Star = (double) 2068889365;
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(2068889365, regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.m_alpha = doubleArray0;
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(2068889365, regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.m_target = doubleArray0;
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(2068889365, regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      try { 
        regSMO0.optimize();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }

  @Test(timeout = 4000)
  public void test60()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      boolean boolean0 = regSMO0.m_bModelBuilt;
      assertFalse(boolean0);
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      double[] doubleArray0 = new double[21];
      double[] doubleArray1 = new double[6];
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      
      regSMO0.m_alphaStar = doubleArray1;
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.m_alpha = doubleArray0;
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.m_target = doubleArray1;
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      String string0 = regSMO0.toString();
      assertEquals("SMOreg\n\nSupport vectors:\n +       0     \n\n\n\nNumber of kernel evaluations: 0", string0);
      assertNotNull(string0);
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      
      regSMO0.optimize();
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO0.getSeed());
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertFalse(regSMO0.modelBuilt());
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
  }

  @Test(timeout = 4000)
  public void test61()  throws Throwable  {
      RegSMO regSMO0 = new RegSMO();
      assertNotNull(regSMO0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      TechnicalInformation technicalInformation0 = regSMO0.getTechnicalInformation();
      assertNotNull(technicalInformation0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(TechnicalInformation.Type.MISC, technicalInformation0.getType());
      assertFalse(technicalInformation0.hasAdditional());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      SMOset sMOset0 = new SMOset(16);
      assertNotNull(sMOset0);
      assertEquals(0, sMOset0.numElements());
      
      regSMO0.m_supportVectors = sMOset0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      assertEquals(0, sMOset0.numElements());
      assertEquals(0, regSMO0.m_supportVectors.numElements());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      
      String[] stringArray0 = sGDText0.getOptions();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(stringArray0);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      
      SMOreg sMOreg0 = regSMO0.m_SVM;
      assertNull(sMOreg0);
      
      regSMO0.m_SVM = null;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertEquals(0, regSMO0.getKernelEvaluations());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = (double) 1;
      doubleArray0[4] = 1.0;
      regSMO0.m_nEvals = 1;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      doubleArray0[5] = (double) 1;
      regSMO0.m_alphaStar = doubleArray0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      String string0 = sMOset0.getRevision();
      assertEquals("8034", string0);
      assertNotNull(string0);
      assertEquals(0, sMOset0.numElements());
      
      doubleArray0[0] = (double) 16;
      sGDText0.setUseWordFrequencies(false);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      
      regSMO0.m_alpha = regSMO0.m_alphaStar;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      regSMO0.m_target = doubleArray0;
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      boolean boolean1 = regSMO0.findOptimalPointOnLine(1, 1, 1, 1.0, 0, 1.7976931346825464E308, 0.0, 0, 0, 839.68037507, 1);
      assertTrue(boolean1);
      assertTrue(boolean1 == boolean0);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      boolean boolean2 = regSMO0.findOptimalPointOnLine(1, 0.0, (-384.929), (-4630.0), 1, 0.0, (-216.360909115799), (-216.360909115799), 1.0, 1.0E-12, 1613.0);
      assertTrue(boolean2);
      assertTrue(boolean2 == boolean0);
      assertTrue(boolean2 == boolean1);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO0.epsilonTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO0.epsilonParameterTipText());
      assertEquals(1, regSMO0.getSeed());
      assertEquals(1, regSMO0.getKernelEvaluations());
      assertEquals(1.0E-12, regSMO0.getEpsilon(), 0.01);
      assertEquals("Seed for random number generator.", regSMO0.seedTipText());
      assertEquals((-1), regSMO0.getCacheHits());
      assertFalse(regSMO0.modelBuilt());
      assertEquals(0.001, regSMO0.getEpsilonParameter(), 0.01);
      
      RegSMO regSMO1 = new RegSMO();
      assertFalse(regSMO1.equals((Object)regSMO0));
      assertNotNull(regSMO1);
      assertEquals("The epsilon for round-off error (shouldn't be changed).", regSMO1.epsilonTipText());
      assertFalse(regSMO1.modelBuilt());
      assertEquals(1.0E-12, regSMO1.getEpsilon(), 0.01);
      assertEquals(0, regSMO1.getKernelEvaluations());
      assertEquals((-1), regSMO1.getCacheHits());
      assertEquals(0.001, regSMO1.getEpsilonParameter(), 0.01);
      assertEquals(1, regSMO1.getSeed());
      assertEquals("Seed for random number generator.", regSMO1.seedTipText());
      assertEquals("The epsilon parameter of the epsilon insensitive loss function.(default 0.001).", regSMO1.epsilonParameterTipText());
      
      try { 
        regSMO1.optimize();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.functions.supportVector.RegSMO", e);
      }
  }
}
